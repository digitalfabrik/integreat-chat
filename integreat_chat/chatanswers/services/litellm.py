"""
Very simple LiteLLM Client (should be compatible to OpenAI API)
"""

import requests

from django.conf import settings

class LiteLLMClient:
    """
    API Client for 
    """
    def __init__(self, system_prompt: str, model: str):
        """
        Initialize the API client with a LLM model

        param system_prompt: A system prompt that provides general orientation to the LLM
        param model: LLM Model
        """
        self.model = model
        self.system_prompt = system_prompt

    def simple_prompt(self, message: str) -> str:
        """
        Simple message and answer function.

        param message: Message prompted to LLM
        return: message returned by LLM
        """
        messages = [{
            "role": "user",
            "content": message
        }]
        return self.chat_prompt(messages)

    def chat_prompt(self, messages: list[dict]) -> str:
        """
        Get RAG answer

        param messages: [{"role": "user"|"assistant","content": "Where can I learn German?"}]
        return: a message generated by an LLM
        """
        url = f"{settings.LITELLM_SERVER}/chat/completions"
        body = {
            "model": self.model,
            "messages": [
            {
                "role": "system",
                "content": self.system_prompt
            }] + messages
        }
        response = requests.post(url, json=body, timeout=120, headers={
            'Authorization': f'Bearer {settings.LITELLM_API_KEY}',
            'Content-Type': 'application/json',
        })
        return response.json()["choices"][0]["message"]["content"]
